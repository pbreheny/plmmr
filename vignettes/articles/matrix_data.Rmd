---
title: "If your data is in a matrix or data frame"
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(plmmr)
```

In this overview, I will provide a demo of the main functions in `plmmr` using the `admix` data. Checkout the other vignettes to see examples of analyzing data from PLINK files or delimited files.

Examine what we have in the `admix` data: 

```{r}
str(admix)
```

## Basic model fitting 

The `admix` dataset is now ready to analyze with a call to `plmmr::plmm()` (one of the main functions in `plmmr`):

```{r admix_fit}
admix_fit <- plmm(admix$X, admix$y)
summary(admix_fit, lambda = admix_fit$lambda[50])
```

Notice: I am passing `admix$X` as the `design` argument in `plmm()`; internally, `plmm()` has taken this `X` input and created a `plmm_design` object. You could also supply `X` and `y` to `create_design()` to make this step explicit. 

The returned `beta_vals` item is a matrix whose rows are $\hat\beta$ coefficients and whose columns represent values of the penalization parameter $\lambda$. By default, `plmm` fits 100 values of $\lambda$ (see the `setup_lambda` function for details). 

```{r}
admix_fit$beta_vals[1:10, 97:100] |> 
  knitr::kable(digits = 3,
               format = "html")
```

Note that for all values of $\lambda$, SNP 8 has $\hat \beta = 0$. This is because SNP 8 is a constant feature, a feature (i.e., a column of $\mathbf{X}$) whose values do not vary among the members of this population.

We can summarize our fit at the nth $\lambda$ value:  

```{r summary1}
# for n = 25 
summary(admix_fit, lambda = admix_fit$lambda[25])
```

We can also plot the path of the fit to see how model coefficients vary with $\lambda$:

```{r admix_plots, fig.align='center', fig.width=8, fig.cap="Plot of path for model fit"}
plot(admix_fit)
```

Suppose we also know the ancestry groups with which for each person in the `admix` data self-identified. We would probably want to include this in the model as an unpenalized covariate (i.e., we would want 'ancestry' to always be in the model). To specify an unpenalized covariate, we need to use the `create_design()` function prior to calling `plmm()`. Here is how that would look: 

```{r}
# add ancestry to design matrix
X_plus_ancestry <- cbind(admix$ancestry, admix$X)

# adjust column names -- need these for designating 'unpen' argument
colnames(X_plus_ancestry) <- c("ancestry", colnames(admix$X))

# create a design
admix_design2 <- create_design(X = X_plus_ancestry,
                               y = admix$y,
                               # below, I mark ancestry variable as unpenalized
                               # we want ancestry to always be in the model
                               unpen = "ancestry")

# now fit a model 
admix_fit2 <- plmm(design = admix_design2)
```

We may compare the results from the model which includes 'ancestry' to our first model: 

```{r}
summary(admix_fit2, idx = 25)
plot(admix_fit2)
```

## Cross validation 

To select a $\lambda$ value, we often use cross validation. Below is an example of using `cv_plmm` to select a $\lambda$ that minimizes cross-validation error: 

```{r admix_cv}

admix_cv <- cv_plmm(design = admix_design2, return_fit = T)

admix_cv_s <- summary(admix_cv, lambda = "min")
print(admix_cv_s)
```

We can also plot the cross-validation error (CVE) versus $\lambda$ (on the log scale):

```{r cvplot, fig.align='center', fig.width=8, fig.cap="Plot of CVE"}
plot(admix_cv)
```

### Parallelization 

As an option for cross-validation, with data stored in-memory you may choose to implement CV in parallel using the `cluster` argument in `cv_plmm()`. Here is an example of setting up CV in parallel: 

```{r}
# make a cluster
num_cores <- 5 # just using 5 cores as a laptop-sized example
cl <- parallel::makeCluster(spec = num_cores)
print(cl) # check to see what kind of cluster you've made 
cv_fit_parallel <- cv_plmm(design = admix_design2,
                           type = "blup",
                           cluster = cl,
                           return_fit = T,
                           trace = FALSE) 

# note: the results closely correspond to the above
summary(cv_fit_parallel)
plot(cv_fit_parallel)
```


## Prediction

Below is an example of the `predict()` methods for PLMMs: 

```{r admix_pred}
# make predictions for select lambda value(s)
y_hat <- predict(object = admix_fit,
                 newX = admix$X,
                 type = "blup",
                 X = admix$X)

```

We can compare these predictions with the predictions we would get from an intercept-only model using mean squared prediction error (MSPE) -- lower is better:

```{r}
# intercept-only (or 'null') model
crossprod(admix$y - mean(admix$y))/length(admix$y)

# our model at its best value of lambda
apply(y_hat, 2, function(c){crossprod(admix$y - c)/length(c)}) -> mse
min(mse)
# ^ across all values of lambda, our model has MSPE lower than the null model
```

We see our model has better predictions than the null.  
